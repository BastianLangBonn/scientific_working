@inproceedings{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y},
booktitle = {Proceedings of the 21st International Conference on Machine Learning (ICML)},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
isbn = {1581138385 (ISBN)},
keywords = {Algorithms,Apprenticeship learning,Bellman's equations,Decision making,Iterative methods,Learning systems,Markov decision process (MDP),Markov processes,Probability,Reinforcement learning,Theorem proving,Vectors},
pages = {1--8},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-14344251217\&partnerID=40},
year = {2004}
}
@article{Aboaf1988,
abstract = {The functionality of robots can be improved by programming them to
learn tasks from practice. Task-level learning can compensate for the
structural modeling errors of the robot's lower-level control systems
and can speed up the learning process by reducing the degrees of freedom
of the models to be learned. The authors demonstrate two general
learning procedures-fixed-model learning and refined-model learning-on a
ball-throwing robot system. Both learning approaches refine the task
command based on the performance error of the system, while they ignore
the intermediate variables separation the lower-level systems. The
authors also provide experimental and theoretical evidence that
task-level learning can improve the functionality of robots},
author = {Aboaf, E.W. and Atkeson, C.G. and Reinkensmeyer, D.J.},
doi = {10.1109/ROBOT.1988.12245},
isbn = {0-8186-0852-8},
journal = {Proceedings. 1988 IEEE International Conference on Robotics and Automation},
title = {{Task-level robot learning}},
year = {1988}
}
@article{Aleotti2006,
abstract = {Trajectory learning is a fundamental component in a robot Programming by Demonstration (PbD) system, where often the very purpose of the demonstration is to teach complex manipulation patterns. However, human demonstrations are inevitably noisy and inconsistent. This paper highlights the trajectory learning component of a PbD system for manipulation tasks encompassing the ability to cluster, select, and approximate human demonstrated trajectories. The proposed technique provides some advantages with respect to alternative approaches and is suitable for learning from both individual and multiple user demonstrations. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Aleotti, J. and Caselli, S.},
doi = {10.1016/j.robot.2006.01.003},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Robot programming by demonstration,Trajectory learning},
number = {5},
pages = {409--413},
title = {{Robust trajectory learning and approximation for robot programming by demonstration}},
volume = {54},
year = {2006}
}
@article{Argall2009,
abstract = {Wepresent a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.},
author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, B},
doi = {DOI: 10.1016/j.robot.2008.10.024},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Robotics Machine learning},
number = {5},
pages = {469--483},
pmid = {21045796},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{Argall2009a,
abstract = {Wepresent a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.},
author = {Argall, Brenna D. and Argall, Brenna D. and Chernova, Sonia and Chernova, Sonia and Veloso, Manuela and Veloso, Manuela and Browning, Brett and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {autonomous systems,learning from demonstration,machine learning,robotics},
number = {5},
pages = {469--483},
pmid = {21045796},
title = {{A survey of robot learning from demonstration}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889008001772},
volume = {57},
year = {2009}
}
@article{Argall2009b,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. © 2008 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
pmid = {21045796},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{Arsenic2004,
abstract = {This paper addresses a broad spectrum of machine learning problems. Actions by embodied agents automatically generate training data for the learning mechanisms, so that a humanoid robot develops categorization autonomously. Cognitive capabilities of the humanoid robot are developmentally created, starting from abilities for detecting, segmenting, and recognizing objects. Such mature abilities are integrated with the deeper developmental learning mechanisms required to create those abilities out of the robot's physical experiences. This work presents strategies for learning task sequences and to recognize objects employed on such tasks from human-robot interaction cues. Learning strategies are also presented for the control of both oscillatory and non-oscillatory movements for the execution of these learned tasks. Self-exploration of the world automatically introduces the robot to new training data.},
author = {Arsenic, A.M.},
doi = {10.1109/IJCNN.2004.1381182},
isbn = {0-7803-8359-1},
issn = {1098-7576},
journal = {2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
title = {{Developmental learning on a humanoid robot}},
volume = {4},
year = {2004}
}
@misc{Atkeson1995,
abstract = {This paper explores a memory-based approach to robot learning, using memory-based neural networks to learn models of the task to be performed. Steinbuch and Taylor presented neural network designs to explicitly store training data and do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. This network design is equivalent to a statistical approach known as locally weighted regression, in which a local model is formed to answer each query, using a weighted regression in which nearby points (similar experiences) are weighted more than distant points (less relevant experiences). We illustrate this approach by describing how it has been used to enable a robot to learn a difficult juggling task.},
author = {Atkeson, C. G. and Schaal, S.},
booktitle = {Neurocomputing},
doi = {10.1016/0925-2312(95)00033-6},
issn = {09252312},
keywords = {Local models,Locally weighted regression,Memory-based,Nearest neighbor,Robot learning},
number = {3},
pages = {243--269},
title = {{Memory-based neural networks for robot learning}},
volume = {9},
year = {1995}
}
@article{Atkeson1986,
abstract = { We present an algorithm that uses trajectory following errors to improve a feedforward command to a robot. This approach to robot learning is based on explicit modeling of the robot; and uses an inverse of the robot model as part of a learning operator which processes the trajectory errors. Results are presented from a successful implementation of this procedure on the MIT Serial Link Direct Drive Arm. The major point of this paper is that more accurate robot models improve trajectory learning performance, and learning algorithms do not reduce the need for good models in robot control.},
author = {Atkeson, C. and McIntyre, J.},
doi = {10.1109/ROBOT.1986.1087423},
isbn = {0818606959},
journal = {Proceedings. 1986 IEEE International Conference on Robotics and Automation},
title = {{Robot trajectory learning through practice}},
volume = {3},
year = {1986}
}
@article{Atkeson1997,
abstract = {The goal of robot learning from demonstration is to have a robot learn from watching a demonstration of the task to be performed. In our approach to learning from demonstration the robot learns a reward function from the demonstration and a task model from repeated attempts to perform the task. A policy is computed based on the learned reward function and task model. Lessons learned from an implementation on an anthropomorphic robot arm using a pendulum swing up task include 1) simply mimicking demonstrated motions is not adequate to perform this task, 2) a task planner can use a learned model and reward function to compute an appropriate policy, 3) this modelbased planning process supports rapid learning, 4) both parametric and nonparametric models can be learned and used, and 5) incorporating a task level direct learning component, which is non-model-based, in addition to the model-based planner, is useful in compensating for structural modeling errors and slow model learning},
author = {Atkeson, Christopher G and Schaal, Stefan},
isbn = {1558604863},
journal = {Learning},
number = {1994},
pages = {12--20},
title = {{Robot learning from demonstration}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.8531\&amp;rep=rep1\&amp;type=pdf},
year = {1997}
}
@article{Atkeson1997a,
abstract = {The goals of robot learning from demonstration is to have a robot learn from watching a demonstration of the task to be performed. In our approach to learning from demonstration the robot learns a reward function from the demonstration and a task model from repeated attempts to perform the task. A policy is computed based on the learned reward function and task model. Lessons learned from an implementation on an anthropomorphic robot arm using a pendulum swing up task include 1) simply mimicking demonstrated motions is not adequate to perform this task, 2) a task planner can use a learned model and reward function to compute an appropriate policy, 3) this model-based planning process supports rapid learning, 4) both parametric and nonparametric models can be learned and used, and 5) incorporating a task level direct learning component, which is non-model-based, in addition to the model-based planner, is useful in compensating for structurl modeling erros and slow model-learning},
author = {Atkeson, Christopher G. and Schaal, Stefan},
doi = {10.1016/j.robot.2004.03.001},
isbn = {1558604863},
issn = {09218890},
journal = {14th International Conference on Machine Learning},
pages = {12--20},
pmid = {11540378},
title = {{Robot learning from demonstration}},
url = {http://wwwiaim.ira.uka.de/users/rogalla/WebOrdnerMaterial/ml-robotlearning.pdf},
year = {1997}
}
@article{Augustsson2002,
abstract = {We demonstrate the first instance of a real on-line robot learning to develop feasible flying (flapping) behavior, using evolution. Here we present the experiments and results of the first use of evolutionary methods for a flying robot. With nature’s own method, evolution, we address the highly non-linear fluid dynamics of flying. The flying robot is constrained in a test bench where timing and movement of wing flapping is evolved to give maximal lifting force. The robot is assembled with standard off-the-shelf R/C servomotors as actuators. The implementation is a con- ventional steady-state linear evolutionary algorithm.},
author = {Augustsson, Peter and Wolff, Krister and Nordin, Peter},
isbn = {1-55860-878-8},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2002), volume},
pages = {1279--1285},
title = {{Creation of a learning, flying robot by means of evolution}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.1561\&amp;rep=rep1\&amp;type=pdf},
year = {2002}
}
@article{Barfoot2006,
abstract = {This paper reports on experiments involving a hexapod robot. Motivated by neurobiological evidence that control in real hexapod insects is distributed leg-wise, we investigated two approaches to learning distributed controllers: genetic algorithms and reinforcement learning. In the case of reinforcement learning, a new learning algorithm was developed to encourage cooperation between legs. Results from both approaches are presented and compared. ?? 2006.},
author = {Barfoot, T. D. and Earon, E. J P and D'Eleuterio, G. M T},
doi = {10.1016/j.robot.2006.04.009},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Coevolution,Distributed control,Genetic algorithm,Hexapod robot,Reinforcement learning},
number = {10},
pages = {864--872},
title = {{Experiments in learning distributed control for a hexapod robot}},
volume = {54},
year = {2006}
}
@inproceedings{Billard2004,
abstract = {The goals of robot learning from demonstration is to have a robot learn from watching a demonstration of the task to be performed. In our approach to learning from demonstration the robot learns a reward function from the demonstration and a task model from repeated attempts to perform the task. A policy is computed based on the learned reward function and task model. Lessons learned from an implementation on an anthropomorphic robot arm using a pendulum swing up task include 1) simply mimicking demonstrated motions is not adequate to perform this task, 2) a task planner can use a learned model and reward function to compute an appropriate policy, 3) this model-based planning process supports rapid learning, 4) both parametric and nonparametric models can be learned and used, and 5) incorporating a task level direct learning component, which is non-model-based, in addition to the model-based planner, is useful in compensating for structurl modeling erros and slow model-learning},
author = {Billard, Aude and Siegwart, Roland},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.001},
isbn = {1558604863},
issn = {09218890},
number = {2-3},
pages = {65--67},
pmid = {11540378},
title = {{Robot learning from demonstration}},
volume = {47},
year = {2004}
}
@inproceedings{Bocsi2013,
abstract = {Robot manipulation tasks require on robot models. When exact physical parameters of the robot are not available, learning robot models from data becomes an appealing alternative. Most learning approaches are formulated in a supervised learning framework and are based on clearly defined training sets. We propose a method that improves the learning process by using additional data obtained from other experiments of the robot or even from experiments with different robot architectures. Incorporating experiences from other experiments requires transfer learning that has been used with success in machine learning. The proposed method can be used for arbitrary robot model, together with any type of learning algorithm. Experimental results indicate that task transfer between different robot architectures is a sound concept. Furthermore, clear improvement is gained on forward kinematics model learning in a task-space control task.},
author = {Bocsi, Botond and Csato, Lehel and Peters, Jan},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2013.6706721},
isbn = {9781467361293},
issn = {2161-4393},
title = {{Alignment-based transfer learning for robot models}},
year = {2013}
}
@inproceedings{Bowling2003,
abstract = {Multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. There has been a great deal of recent research on multiagent reinforcement learning in stochastic games, which is the intuitive extension of MDPs to multiple agents. This recent work, although general, has only been applied to small games with at most hundreds of states. On the other hand robot tasks have continuous, and often complex, state and action spaces. Robot learning tasks demand approximation and generalization techniques, which have only received extensive attention in single-agent learning. In this paper we introduce GraWoLF, a general-purpose, scalable, multiagent learning algorithm. It combines gradient-based policy learning techniques with the WoLF ("Win or Learn Fast") variable learning rate.},
author = {Bowling, Michael and Veloso, Manuela},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
issn = {10450823},
pages = {699--704},
title = {{Simultaneous adversarial multi-robot learning}},
year = {2003}
}
@article{Cakmak2010,
abstract = {Social learning in robotics has largely focused on imitation learning. Here we take a broader view and are in- terested in the multifaceted ways that a social partner can influence the learning process. We implement four social learning mechanisms on a robot: stimulus enhancement, em- ulation, mimicking, and imitation, and illustrate the compu- tational benefits of each. In particular,we illustrate that some strategies are about directing the attention of the learner to objects and others are about actions. Taken together these strategies form a rich repertoire allowing social learners to use a social partner to greatly impact their learning process. We demonstrate these results in simulation and with physi- cal robot ‘playmates’.},
author = {Cakmak, Maya and Depalma, Nick and Arriaga, Rosa I. and Thomaz, Andrea L.},
doi = {10.1007/s10514-010-9197-9},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Biologically inspired learning,Learning by imitation,Social learning},
number = {3-4},
pages = {309--329},
title = {{Exploiting social partners in robot learning}},
volume = {29},
year = {2010}
}
@inproceedings{Cakmak2009,
abstract = {Social learning in robotics has largely focused on imitation learning. Here we take a broader view and are interested in the multifaceted ways that a social partner can influence the learning process. We implement four social learning mechanisms on a robot: stimulus enhancement, emulation, mimicking, and imitation, and illustrate the computational benefits of each. In particular, we illustrate that some strategies are about directing the attention of the learner to objects and others are about actions. Taken together these strategies form a rich repertoire allowing social learners to use a social partner to greatly impact their learning process. We demonstrate these results in simulation and with physical robot `playmates'.},
author = {Cakmak, Maya and DePalma, Nick and Thomaz, Andrea L. and Arriaga, Rosa},
booktitle = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2009.5326168},
isbn = {9781424450817},
issn = {1944-9445},
pages = {128--134},
title = {{Effects of social exploration mechanisms on robot learning}},
year = {2009}
}
@inproceedings{Cakmak2012,
abstract = {Programming new skills on a robot should takeminimal time and effort. One approach to achieve this goal is to allow the robot to ask questions. This idea, called Active Learning, has recently caught a lot of attention in the robotics commu- nity. However, it has not been explored from a human-robot interaction perspective. In this paper, we identify three types of questions (label, demonstration and feature queries) and discuss how a robot can use these while learning new skills. Then, we present an experiment on human question asking which characterizes the extent to which humans use these question types. Finally, we evaluate the three question types within a human-robot teaching interaction. We inves- tigate the ease with which different types of questions are answered and whether or not there is a general preference of one type of question over another. Based on our findings from both experiments we provide guidelines for designing question asking behaviors on a robot learner.},
author = {Cakmak, Maya and Thomaz, Andrea L},
booktitle = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
doi = {10.1145/2157689.2157693},
isbn = {9781450310635},
issn = {2167-2121},
keywords = {active learning,learning from demonstration},
pages = {17},
title = {{Designing robot learners that ask good questions}},
year = {2012}
}
@inproceedings{Calinon2007,
abstract = {We present an approach to teach incrementally human ges- tures to a humanoid robot. The learning process consists of first projecting the movement data in a latent space and en- coding the resulting signals in a Gaussian Mixture Model (GMM). We compare the performance of two incremen- tal training procedures against a batch training procedure. Qualitative and quantitative evaluations are performed on data acquired from motion sensors attached to a human demonstrator and data acquired by kinesthetically demon- strating the task to the robot. We present experiments to show that these different modalities can be used to teach incrementally basketball officials’ signals to a HOAP-3 hu- manoid robot.},
author = {Calinon, Sylvain and Billard, Aude},
booktitle = {International conference on Human-robot interaction},
doi = {10.1145/1228716.1228751},
isbn = {9781595936172},
pages = {255},
title = {{Incremental learning of gestures by imitation in a humanoid robot}},
url = {http://portal.acm.org/citation.cfm?doid=1228716.1228751},
year = {2007}
}
@article{Calinon2007a,
abstract = {We present a programming-by-demonstration framework for generically extracting the relevant features of a given task and for addressing the problem of generalizing the acquired knowledge to different contexts. We validate the architecture through a series of experiments, in which a human demonstrator teaches a humanoid robot simple manipulatory tasks. A probability-based estimation of the relevance is suggested by first projecting the motion data onto a generic latent space using principal component analysis. The resulting signals are encoded using a mixture of Gaussian/Bernoulli distributions (Gaussian mixture model/Bernoulli mixture model). This provides a measure of the spatio-temporal correlations across the different modalities collected from the robot, which can be used to determine a metric of the imitation performance. The trajectories are then generalized using Gaussian mixture regression. Finally, we analytically compute the trajectory which optimizes the imitation metric and use this to generalize the skill to different contexts.},
author = {Calinon, Sylvain and Guenter, Florent and Billard, Aude},
doi = {10.1109/TSMCB.2006.886952},
isbn = {1083-4419},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Gaussian mixture model (GMM),Human motion subspace,Human-robot interaction (HRI),Learning by imitation,Metric of imitation,Programming by demonstration (PbD)},
number = {2},
pages = {286--298},
pmid = {17416157},
title = {{On learning, representing, and generalizing a task in a humanoid robot}},
volume = {37},
year = {2007}
}
@inproceedings{Calinon2010,
abstract = {We propose a control strategy for a robotic manipulator operating in an unstructured environment while interacting with a human operator. The proposed system takes into account the important characteristics of the task and the redundancy of the robot to determine a controller that is safe for the user. The constraints of the task are first extracted using several examples of the skill demonstrated to the robot through kinesthetic teaching. An active control strategy based on task-space control with variable stiffness is proposed, and combined with a safety strategy for tasks requiring humans to move in the vicinity of robots. A risk indicator for human-robot collision is defined, which modulates a repulsive force distorting the spatial and temporal characteristics of the movement according to the task constraints. We illustrate the approach with two human-robot interaction experiments, where the user teaches the robot first how to move a tray, and then shows it how to iron a napkin.},
author = {Calinon, Sylvain and Sardellitti, Irene and Caldwell, Darwin G.},
booktitle = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
doi = {10.1109/IROS.2010.5648931},
isbn = {9781424466757},
issn = {2153-0858},
pages = {249--254},
title = {{Learning-based control strategy for safe human-robot interaction exploiting task and robot redundancies}},
year = {2010}
}
@inproceedings{Cantrell2011,
abstract = {Natural language interactions between humans and robots are currently limited by many factors, most notably by the robot's concept representations and action repertoires. We propose a novel algorithm for learning meanings of action verbs through dialogue-based natural language descriptions. This functionality is deeply integrated in the robot's natural language subsystem and allows it to perform the actions associated with the learned verb meanings right away without any additional help or learning trials. We demonstrate the effectiveness of the algorithm in a scenario where a human explains to a robot the meaning of an action verb unknown to the robot and the robot is subsequently able to carry out the instructions involving this verb.},
author = {Cantrell, Rehj and Schermerhorn, Paul and Scheutz, Matthias},
booktitle = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2011.6005199},
isbn = {9781457715716},
pages = {125--130},
title = {{Learning actions from human-robot dialogues}},
year = {2011}
}
@inproceedings{Casarrubias-Vargas2010,
abstract = {In this work we propose the use of machine learning techniques to improve Simultaneous Localization and Mapping (SLAM) using an extended Kalman filter (EKF) and visual information for robot navigation. We are using the Viola and Jones approach for looking specific visual landmarks in environment. The landmarks are used to improve the robot localization in the EKF-SLAM system. Our experiments validate the efficiency of our algorithm.},
author = {Casarrubias-Vargas, H. and Petrilli-Barcel\'{o}, A. and Bayro-Corrochano, E.},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.105},
isbn = {9780769541099},
issn = {10514651},
pages = {396--399},
pmid = {5597815},
title = {{EKF-SLAM and machine learning techniques for visual robot navigation}},
year = {2010}
}
@article{Chatzis2012,
abstract = {In the past years, many authors have considered application of machine learning methodologies to effect robot learning by demonstration. Gaussian mixture regression (GMR) is one of the most successful methodologies used for this purpose. A major limitation of GMR models concerns automatic selection of the proper number of model states, i.e., the number of model component densities. Existing methods, including likelihood- or entropy-based criteria, usually tend to yield noisy model size estimates while imposing heavy computational requirements. Recently, Dirichlet process (infinite) mixture models have emerged in the cornerstone of nonparametric Bayesian statistics as promising candidates for clustering applications where the number of clusters is unknown a priori. Under this motivation, to resolve the aforementioned issues of GMR-based methods for robot learning by demonstration, in this paper we introduce a nonparametric Bayesian formulation for the GMR model, the Dirichlet process GMR model. We derive an efficient variational Bayesian inference algorithm for the proposed model, and we experimentally investigate its efficacy as a robot learning by demonstration methodology, considering a number of demanding robot learning by demonstration scenarios. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Chatzis, Sotirios P. and Korkinof, Dimitrios and Demiris, Yiannis},
doi = {10.1016/j.robot.2012.02.005},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Dirichlet process,Gaussian mixture regression,Nonparametric statistics,Robot learning by demonstration,Variational Bayes},
number = {6},
pages = {789--802},
title = {{A nonparametric Bayesian approach toward robot learning by demonstration}},
volume = {60},
year = {2012}
}
@incollection{Connell1993,
abstract = {In this chapter we provide an overview of the field of robot learning. We first discuss why robot learning is interesting and explain what is hard about it. We then characterize the robot learning problem and point out some major issues that need to be addressed. Next we survey some established techniques which are relevant to robot learning and then go on to review some of the recent research in robot learning. Finally, we briefly summarize the contents of the rest of the chapters in this book.},
author = {Connell, Jonathan H and Mahadevan, Sridhar},
booktitle = {Robot Learning},
isbn = {9781461531845},
pages = {1--17},
title = {{Introduction to Robot Learning}},
url = {http://dx.doi.org/10.1007/978-1-4615-3184-5\_1},
volume = {233},
year = {1993}
}
@inproceedings{Crick2011,
abstract = {We present a study of using a robotic learning from demonstration system capable of collecting large amounts of human-robot interaction data through a web-based interface. We examine the effect of different perceptual mappings between the human teacher and robot on the learning from demonstration. We show that humans are significantly more effective at teaching a robot to navigate a maze when presented with information that is limited to the robot's perception of the world, even though their task performance measurably suffers when contrasted with users provided with a natural and detailed raw video feed. Robots trained on such demonstrations learn more quickly, perform more accurately and generalize better. We also demonstrate a set of software tools for enabling internet-mediated human-robot interaction and gathering the large datasets that such crowdsourcing makes possible.},
author = {Crick, Christopher and Osentoski, Sarah and Jay, Graylin and Jenkins, Odest Chadwicke O.C.},
booktitle = {Proceedings of the 6th international conference on Human-robot interaction - HRI '11},
doi = {10.1145/1957656.1957788},
isbn = {9781450305617},
issn = {2167-2121},
pages = {339--346},
title = {{Human and robot perception in large-scale learning from demonstration}},
url = {http://dl.acm.org/citation.cfm?id=1957656.1957788},
year = {2011}
}
@article{DSouza2001,
abstract = {Real-time control of the end-effector of a humanoid robot in
external coordinates requires computationally efficient solutions of the
inverse kinematics problem. In this context, this paper investigates
inverse kinematics learning for resolved motion rate control (RMRC)
employing an optimization criterion to resolve kinematic redundancies.
Our learning approach is based on the key observations that learning an
inverse of a nonuniquely invertible function can be accomplished by
augmenting the input representation to the inverse model and by using a
spatially localized learning approach. We apply this strategy to inverse
kinematics learning and demonstrate how a recently developed statistical
learning algorithm, locally weighted projection regression, allows
efficient learning of inverse kinematic mappings in an incremental
fashion even when input spaces become rather high dimensional. Our
results are illustrated with a 30-DOF humanoid robot},
author = {D'Souza, A. and Vijayakumar, S. and Schaal, S.},
doi = {10.1109/IROS.2001.973374},
isbn = {0-7803-6612-3},
issn = {1941-0492},
journal = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
pmid = {19022727},
title = {{Learning inverse kinematics}},
volume = {1},
year = {2001}
}
@inproceedings{DeGreeff2012,
abstract = {In this paper we describe how a robot may benefit from active learning in a human-robot tutelage setting. Rather than passively absorbing conceptual knowledge, the robot learner actively tries to influence the human teacher in order to improve its learning experience. We compare the performance of agents that employ this strategy in simulation to a robot that interacts with a human teacher. It was found that people respond to the robot's social cues and that this can improve learning, albeit less expressed than in simulation. Moreover, we found gender effects indicating that robot learners might benefit from even more specific tailoring towards their human tutors.},
author = {{De Greeff}, Joachim and Delaunay, Fr\'{e}d\'{e}ric and Belpaeme, Tony},
booktitle = {2012 IEEE International Conference on Development and Learning and Epigenetic Robotics, ICDL 2012},
doi = {10.1109/DevLrn.2012.6400838},
isbn = {9781467349635},
title = {{Active robot learning with human tutelage}},
year = {2012}
}
@inproceedings{Dillmann2004,
abstract = {Within this paper, an approach for teaching a humanoid robot is presented that will enable the robot to learn typical tasks required in everyday household environments. Our approach, called Programming by Demonstration, which is implemented and successfully used in our institute to teach a robot system is presented. Firstly, we concentrate on an analysis of human actions and action sequences that can be identified when watching a human demonstrator. Secondly, sensor aid systems are introduced which augment the robot's perception capabilities while watching a human's demonstration and the robot's execution of tasks respectively. The main focus is then layed on the knowledge representation in order to be able to abstract the problem solution strategies and to transfer them onto the robot system. © 2004 Elsevier B.V. All rights reserved.},
author = {Dillmann, R\"{u}diger},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.005},
isbn = {09218890},
issn = {09218890},
keywords = {One-Shot-Learning,Programming by Demonstration,Robot tasks},
number = {2-3},
pages = {109--116},
title = {{Teaching and learning of robot tasks via observation of human performance}},
volume = {47},
year = {2004}
}
@article{Engelson1992,
abstract = {An issue that must be addressed in map-learning systems is that of
error accumulation. The primary emphasis in the literature has been on
reducing errors entering the map. The authors suggest that this
methodology must reach a point of diminishing returns, and hence focus
on explicit error detection and correction. By identifying the possible
types of mapping errors, structural constraints can be exploited to
detect and diagnose mapping errors. Such robust mapping requires little
overhead beyond that needed for nonrobust mapping. A mapping system was
implemented based on those ideas. Extensive testing in simulation
demonstrated the effectiveness of the proposed error-correction
strategies},
author = {Engelson, S.P. and McDermott, D.V.},
doi = {10.1109/ROBOT.1992.220057},
isbn = {0-8186-2720-4},
journal = {Proceedings 1992 IEEE International Conference on Robotics and Automation},
title = {{Error correction in mobile robot map learning}},
year = {1992}
}
@article{Erden2008,
abstract = {In this paper the problem of free gait generation and adaptability with reinforcement learning are addressed for a six-legged robot. Using the developed free gait generation algorithm the robot maintains to generate stable gaits according to the commanded velocity. The reinforcement learning scheme incorporated into the free gait generation makes the robot choose more stable states and develop a continuous walking pattern with a larger average stability margin. While walking in normal conditions with no external effects causing unstability, the robot is guaranteed to have stable walk, and the reinforcement learning only improves the stability. The adaptability of the learning scheme is tested also for the abnormal case of deficiency in one of the rear-legs. The robot gets a negative reinforcement when it falls, and a positive reinforcement when a stable transition is achieved. In this way the robot learns to achieve a continuous pattern of stable walk with five legs. The developed free gait generation with reinforcement learning is applied in real-time on the actual robot both for normal walking with different speeds and learning of five-legged walking in the abnormal case. © 2007 Elsevier Ltd. All rights reserved.},
author = {Erden, Mustafa Suphi and Leblebicioǧlu, Kemal},
doi = {10.1016/j.robot.2007.08.001},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Free gait,Locomotion,Reinforcement learning,Six-legged robot,Walking},
number = {3},
pages = {199--212},
title = {{Free gait generation with reinforcement learning for a six-legged robot}},
volume = {56},
year = {2008}
}
@article{Ertel2009,
abstract = {There exist many powerful machine learning software libraries, which help the engineer to build robots that learn autonomously. However, engineering of an autonomous robot still is a challenging and time consuming task even with these learning libraries. With the open source Teaching-Box presented here, the ldquotrainingrdquo of a robot becomes easier due to the following features. The Java library of the Teaching-Box provides algorithms for reinforcement learning as well as for learning by demonstration (utilizing supervised learning algorithms) and data structures for exchanging policies between the different ways of learning. As an initial policy one can even take a manually coded behaviour and then improve it for example with reinforcement learning. A human trainer feedback (e.g. via the speech interface) can be used to increase the learning speed. The Eclipse based GUI facilitates the design of the robot learning projects and visualizes the learning process. For connecting the various modules of a project, open interface standards such as RL-Glue are used and an easy integration of the Teaching-Box into standard robot middleware is possible.},
author = {Ertel, W. and Schneider, M. and Cubek, R. and Tokicy, M.},
isbn = {978-1-4244-4855-5},
journal = {2009 International Conference on Advanced Robotics},
title = {{The Teaching-Box: A universal robot learning framework}},
year = {2009}
}
@article{Fernandez2005,
abstract = {Reinforcement learning has been widely applied to solve a diverse set of learning tasks, from board games to robot behaviours. In some of them, results have been very successful, but some tasks present several characteristics that make the application of reinforcement learning harder to define. One of these areas is multi-robot learning, which has two important problems. The first is credit assignment, or how to define the reinforcement signal to each robot belonging to a cooperative team depending on the results achieved by the whole team. The second one is working with large domains, where the amount of data can be large and different in each moment of a learning step. This paper studies both issues in a multi-robot environment, showing that introducing domain knowledge and machine learning algorithms can be combined to achieve successful cooperative behaviours.},
author = {Fern\'{a}ndez, Fernando and Borrajo, Daniel and Parker, Lynne E.},
doi = {10.1007/s10846-005-5137-x},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Collaborative multi-robot domains,Function approximation,Reinforcement learning,State space discretizations},
number = {2-4},
pages = {161--174},
title = {{A reinforcement learning algorithm in cooperative multi-robot domains}},
volume = {43},
year = {2005}
}
@article{Franz1998,
abstract = {We present a purely vision-based scheme for learning a topological representation of an open environment. The system represents selected places by local views of the surrounding scene, and finds traversable paths between them. The set of recorded views and their connections are combined into a graph model of the environment. To navigate between views connected in the graph, we employ a homing strategy inspired by findings of insect ethology. In robot experiments, we demonstrate that complex visual exploration and navigation tasks can thus be performed without using metric information.},
author = {Franz, M O and Sch\"{o}lkopf, B and Mallot, H A and B\"{u}lthoff, H H},
doi = {10.1145/267658.267687},
isbn = {0897918770},
issn = {09295593},
journal = {Autonomous Robots},
number = {1},
pages = {111--125},
title = {{Learning view graphs for robot navigation}},
url = {http://portal.acm.org/citation.cfm?doid=267658.267687},
volume = {5},
year = {1998}
}
@misc{Gadanho2001,
abstract = {The adaptive value of emotions in nature indicates that they might also be useful in artificial creatures. Experiments were carried out to investigate this hypothesis in a simulated learning robot. For this purpose, a non-symbolic emotion model was developed that takes the form of a recurrent artificial neural network where emotions both depend on and influence the perception of the state of the world. This emotion model was integrated in a reinforcement-learning architecture with three different roles: influencing perception, providing reinforcement value, and determining when to reevaluate decisions. Experiments to test and compare this emotion-dependent architecture with a more conventional architecture were done in the context of a solitary learning robot performing a survival task. This research led to the conclusion that artificial emotions are a useful construct to have in the domain of behavior-based autonomous agents with multiple goals and faced with an unstructured environment, because they provide a unifying way to tackle different issues of control, analogous to natural systems' emotions.},
author = {Gadanho, S. C. and Hallam, J.},
booktitle = {Adaptive Behavior},
doi = {10.1177/105971230200900102},
issn = {1059-7123},
number = {1},
pages = {42--64},
title = {{Robot Learning Driven by Emotions}},
volume = {9},
year = {2001}
}
@inproceedings{Gopalakrishnan2005,
abstract = { This research develops a vision-based learning mechanism for semi-autonomous mobile robot navigation. Laser-based localization, vision-based object detection and recognition, and route-based navigation techniques for a mobile robot have been integrated. Initially, the robot can localize itself in an indoor environment with its laser range finder. Then, a user can teleoperate the robot and point the objects of interest via a graphical user interface. In addition, the robot can automatically detect potential objects of interest. The objects are automatically recognized by the object recognition system using neural networks. If the robot cannot recognize an object, it asks the user to identify it. The user can ask the robot to navigate back autonomously to an object recognized or identified before. The human and robot can interact vocally via an integrated speech recognition and synthesis software component. The completed system has been successfully tested on a Pioneer 3-AT mobile robot.},
author = {Gopalakrishnan, Arati and Greene, Sheldon and Sekmen, Ali},
booktitle = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2005.1513755},
isbn = {0780392752},
keywords = {Human-robot interaction,Mobile robot navigation,Object recognition},
pages = {48--53},
title = {{Vision-based mobile robot learning and navigation}},
volume = {2005},
year = {2005}
}
@inproceedings{Grollman2008,
abstract = {We are interested in transferring control policies for arbitrary tasks from a human to a robot. Using interactive demonstration via teleoperation as our transfer scenario, we cast learning as statistical regression over sensor-actuator data pairs. Our desire for interactive learning necessitates algorithms that are incremental and realtime. We examine locally weighted projection regression, a popular robotic learning algorithm, and sparse online Gaussian processes in this domain on one synthetic and several robot-generated data sets. We evaluate each algorithm in terms of function approximation, learned task performance, and scalability to large data sets.},
author = {Grollman, Daniel H and Jenkins, Odest Chadwicke},
booktitle = {2008 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2008.4543716},
isbn = {978-1-4244-1646-2},
issn = {1050-4729},
keywords = {Educational robots,Function approximation,Gaussian processes,Ground penetrating radar,Human robot interaction,Machine learning algorithms,Robot control,Robot programming,Robot sensing systems,Robotics and automation,interactive robot control policy estimation,learning (artificial intelligence),locally weighted projection regression,regression analysis,robotic learning algorithm,robots,sparse incremental learning,sparse online Gaussian process,statistical regression,teleoperation},
pages = {3315--3320},
title = {{Sparse incremental learning for interactive robot control policy estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4543716},
year = {2008}
}
@inproceedings{Grollman2007,
abstract = {We seek to enable users to teach personal robots arbitrary tasks so that the robot can better perform as the user desires without explicit programming. Robot learning from demonstration is an approach well-suited to this paradigm, as a robot learns new tasks from observations of the task itself. Many current robot learning algorithms require the existence of basic behaviors that can be combined to perform the desired task. However, robots that exist in the world for long timeframes and learn many tasks over their lifetime may exhaust this basis set and need to move beyond it. In particular, we are interested in a robot that must learn to perform an unknown task for which its built in behaviors may not be appropriate. We demonstrate a learning paradigm that is capable of learning both low-level motion primitives (locomotion and manipulation) and high-level tasks built on top of them from interactive demonstration. We apply nonparametric regression within this framework towards learning a complete robot soccer player and successfully teach a robot dog to first walk, and then to seek and acquire a ball.},
author = {Grollman, Daniel H. and Jenkins, Odest Chadwicke},
booktitle = {2007 IEEE 6th International Conference on Development and Learning, ICDL},
doi = {10.1109/DEVLRN.2007.4354062},
isbn = {1424411165},
pages = {276--281},
title = {{Learning robot soccer skills from demonstration}},
year = {2007}
}
@inproceedings{Gu2011,
abstract = {This paper proposes a two-phase learning framework for human-robot collaborative manipulation tasks. A table-lifting task performed jointly by a human and a humanoid robot is considered. In order to perform the task, the robot should learn to hold the table at a suitable position and then perform the lifting task cooperatively with the human. Accordingly, learning is split into two phases. The first phase enables the robot to reach out and hold one end of the table. A Programming by Demonstration (PbD) algorithm based on GMM/GMR is used to accomplish this. In the second phase the robot switches its role to an agent learning to collaborate with the human on the task. A guided reinforcement learning algorithm is developed. Using the proposed framework, the robot can successfully learn to reach and hold the table and keep the table horizontal during lifting it up with human in a reasonable amount of time.},
author = {Gu, Ye and Thobbi, Anand and Sheng, Weihua},
booktitle = {2011 IEEE International Conference on Information and Automation, ICIA 2011},
doi = {10.1109/ICINFA.2011.5948979},
isbn = {9781457702686},
keywords = {Cooperative Manipulation,Human-Robot Collaboration,Humanoids,Imitation learning,Reinforcement learning},
pages = {151--156},
title = {{Human-robot collaborative manipulation through imitation and reinforcement learning}},
year = {2011}
}
@article{Han2009,
abstract = {ensino apoiado por rob\^{o}s, Cor\'{e}ia, 2009, robot ministra um conjunto de atividades de ensino na disciplina de idioma ingl\^{e}s para 117 crian\c{c}as, o experimento indica que as crian\c{c}as desenvolveral algo como um vinculo de relacionamento com o rob\^{o}.},
author = {Han, Jeonghye and Kim, Dongho},
doi = {10.1145/1514095.1514163},
isbn = {9781605584041},
issn = {2167-2121},
journal = {Proceedings of the 4th ACM/IEEE international conference on Human robot interaction - HRI '09},
keywords = {3,a,e-learning,personal relationship,r-learning,r-learning contents of t,robot service,teaching assistant robot},
pages = {255},
title = {{r-Learning services for elementary school students with a teaching assistant robot}},
url = {http://portal.acm.org/citation.cfm?doid=1514095.1514163},
year = {2009}
}
@article{Hara2004,
abstract = { It is pointed out that human-robot interface or human interface must be newly interpreted as intelligence of communicative interaction between robot and human, and is again a key issue for development of a new species of robot that is able to serve humans. We point out a concept of "active human interface (or media) " to be composed of three functions and essential to such new robots. When applying such machine to use for human service, psychological familiarity of robot form is another issue to ensure the psychological acceptance. We refer again to the existence of uncanny valley in familiarity to robot appearance or form. We have been developing a life-like face robot that has been a human media for realizing such intelligence of communicative interaction and technological issues and performance of face robot's recognition function of human emotion and actuation function of facial expressions are briefly summarized to understand the state-of-the arts about the face robot Mark II. Then basic studies for characterizing face robot behavior through communicative interactions with a human partner are described by showing two test results of imitation of facial expressions and personality creation in face robot response. We point out the importance of learning method employed by face robot in communicative interactions is also key issue. We can show that the most suitable learning method for human partner is OR type supervised reinforced one. Finally we discuss the value system in relation to unsupervised learning of face robot in communicative interaction to characterize its emotion or selection criteria or bias at taking a certain response facial expression to the partner's state of mind. At the end concluding remarks and future studies are briefly pointed out.},
author = {Hara, F.},
doi = {10.1109/ROMAN.2004.1374712},
isbn = {0-7803-8570-5},
journal = {RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)},
title = {{Artificial emotion of face robot through learning in communicative interactions with human}},
year = {2004}
}
@article{Hersch2008,
abstract = {We present a system for robust robot skill acquisition from kinesthetic demonstrations. This system allows a robot to learn a simple goal-directed gesture and correctly reproduce it despite changes in the initial conditions and perturbations in the environment. It combines a dynamical system control approach with tools of statistical learning theory and provides a solution to the inverse kinematics problem when dealing with a redundant manipulator. The system is validated on two experiments involving a humanoid robot: putting an object into a box and reaching for and grasping an object.},
author = {Hersch, Micha and Guenter, Florent and Calinon, Sylvain and Billard, Aude},
doi = {10.1109/TRO.2008.2006703},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Dynamical system control,Gaussian mixture regression,Hybrid joint and end-effector control,Intelligent robots,Manipulators,Robot programming by demonstration (PbD),Simple robotic manipulation},
number = {6},
pages = {1463--1467},
title = {{Dynamical system modulation for robot learning via kinesthetic demonstrations}},
volume = {24},
year = {2008}
}
@article{Hester2010,
abstract = {Reinforcement learning (RL) algorithms have long been promising methods for enabling an autonomous robot to improve its behavior on sequential decision-making tasks. The obvious enticement is that the robot should be able to improve its own behavior without the need for detailed step-by-step programming. However, for RL to reach its full potential, the algorithms must be sample efficient: they must learn competent behavior from very few real-world trials. From this perspective, model-based methods, which use experiential data more efficiently than model-free approaches, are appealing. But they often require exhaustive exploration to learn an accurate model of the domain. In this paper, we present an algorithm, Reinforcement Learning with Decision Trees (RL - DT that uses decision trees to learn the model by generalizing the relative effect of actions across states. The agent explores the environment until it believes it has a reasonable policy. The combination of the learning approach with the targeted exploration policy enables fast learning of the model. We compare RL - DT against standard model-free and model-based learning methods, and demonstrate its effectiveness on an Aldebaran Nao humanoid robot scoring goals in a penalty kick scenario.},
author = {Hester, Todd and Quinlan, Michael and Stone, Peter},
doi = {10.1109/ROBOT.2010.5509181},
isbn = {9781424450404},
issn = {10504729},
journal = {East},
number = {May},
pages = {2369--2374},
title = {{Generalized Model Learning for Reinforcement Learning on a Humanoid Robot}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5509181},
year = {2010}
}
@article{Ikemoto2012,
abstract = {Close physical interaction between robots and humans is a particularly challenging aspect of robot development. For successful interaction and cooperation, the robot must have the ability to adapt its behavior to the human counterpart. Based on our earlier work, we present and evaluate a computationally efficient machine learning algorithm that is well suited for such close-contact interaction scenarios. We show that this algorithm helps to improve the quality of the interaction between a robot and a human caregiver. To this end, we present two human-in-the-loop learning scenarios that are inspired by human parenting behavior, namely, an},
author = {Ikemoto, Shuhei and Amor, Heni Ben and Minato, Takashi and Jung, Bernhard and Ishiguro, Hiroshi},
doi = {10.1109/MRA.2011.2181676},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {4},
pages = {24--35},
title = {{Physical human-robot interaction: Mutual learning and adaptation}},
volume = {19},
year = {2012}
}
@article{Kartoun2010,
abstract = {{This paper presents a new reinforcement learning algorithm that enables collaborative learning between a robot and a human. The algorithm which is based on the Q(lambda) approach expedites the learning process by taking advantage of human intelligence and expertise. The algorithm denoted as CQ(lambda) provides the robot with self awareness to adaptively switch its collaboration level from autonomous (self performing, the robot decides which actions to take, according to its learning function) to semi-autonomous (a human advisor guides the robot and the robot combines this knowledge into its learning function). This awareness is represented by a self test of its learning performance. The approach of variable autonomy is demonstrated and evaluated using a fixed-arm robot for finding the optimal shaking policy to empty the contents of a plastic bag. A comparison between the CQ(lambda) and the traditional Q(lambda)-reinforcement learning algorithm, resulted in faster convergence for the CQ(lambda) collaborative reinforcement learning algorithm.\}},
author = {Kartoun, Uri and Stern, Helman and Edan, Yael},
doi = {10.1007/s10846-010-9422-y},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Human-robot collaboration,Reinforcement learning,Robot learning},
number = {2},
pages = {217--239},
title = {{A human-robot collaborative reinforcement learning algorithm}},
volume = {60},
year = {2010}
}
@article{Kasper2001,
abstract = {Autonomous mobile robots (AMRs), to be truly flexible, should be equipped with learning capabilities, which allow them to adapt effectively to a dynamic and changing environment. This paper proposes a modular, behavior-based control architecture, which is particularly suited for `Learning from Demonstration' experiments in the spatial domain. The robot learns sensory-motor behaviors online by observing the actions of a person, another robot or another behavior. Offline learning phases are not necessary but might be used to trim the attained representation. First results applying RBF-approximation, growing neural cell structures and probabilistic models for progress estimation, are presented.},
author = {Kasper, Michael and Fricke, Gernot and Steuernagel, Katja and {Von Puttkamer}, Ewald},
doi = {10.1016/S0921-8890(00)00119-6},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
number = {2-3},
pages = {153--164},
title = {{Behavior-based mobile robot architecture for Learning from Demonstration}},
volume = {34},
year = {2001}
}
@inproceedings{Kira2010,
abstract = {ABSTRACT We introduce the novel problem of inter - robot transfer learn- ing for perceptual classification of objects, where multiple heterogeneous robots communicate and transfer learned ob- ject models consisting of a fusion of multiple object prop- erties. Unlike traditional transfer learning , ...},
author = {Kira, Zsolt},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1 - Volume 1},
isbn = {9781617387715},
issn = {15582914},
keywords = {inter-robot transfer,multi-robot sys-,transfer learning},
pages = {13--20},
title = {{Inter-Robot Transfer Learning for Perceptual Classification}},
year = {2010}
}
@article{Kirsch2009,
abstract = {One central property of cognitive systems is the ability to learn and to improve continually. We present a robot control language that combines programming and learning in order to make learning executable in the normal robot program. The language constructs of our learning language RoLL rely on the concept of hierarchical hybrid automata to enable a declarative, explicit specification of learning problems. Using the example of an autonomous household robot, we point out some instances where learning-and especially continued learning-makes the robot control program more cognitive. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Kirsch, Alexandra},
doi = {10.1016/j.robot.2009.05.001},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Cognitive systems,Hybrid automata,Robot control language,Robot learning},
number = {9},
pages = {943--954},
title = {{Robot learning language - Integrating programming and learning for cognitive systems}},
volume = {57},
year = {2009}
}
@inproceedings{Kober2011,
abstract = {Many complex robot motor skills can be represented using elementary movements, and there exist efficient techniques for learning parametrized motor plans using demonstrations and self-improvement. However, in many cases, the robot currently needs to learn a new elementary movement even if a parametrized motor plan exists that covers a similar, related situation. Clearly, a method is needed that modulates the elementary movement through the meta-parameters of its representation. In this paper, we show how to learn such mappings from circumstances to
meta-parameters using reinforcement learning. We introduce an appropriate reinforcement learning algorithm based on a kernelized version of the reward-weighted regression. We compare this algorithm to several previous methods on a toy example and show that it performs well in comparison to standard algorithms. Subsequently, we show two robot applications of the presented setup; i.e., the generalization of throwing movements in darts, and of hitting movements in table tennis. We show that both tasks can be learned successfully using simulated and real robots.},
author = {Kober, Jens and Oztop, Erhan and Peters, Jan},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-441},
isbn = {9781577355120},
issn = {10450823},
pages = {2650--2655},
title = {{Reinforcement learning to adjust robot movements to new situations}},
year = {2011}
}
@article{Koenig2010,
abstract = {Inexpensive personal robots will soon become available to a large portion of the population. Currently, most consumer robots are relatively simple single-purpose machines or toys. In order to be cost effective and thus widely accepted, robots will need to be able to accomplish a wide range of tasks in diverse conditions. Learning these tasks from demonstrations offers a convenient mechanism to customize and train a robot by transferring task related knowledge from a user to a robot. This avoids the time-consuming and complex process of manual programming. The way in which the user interacts with a robot during a demonstration plays a vital role in terms of how effectively and accurately the user is able to provide a demonstration. Teaching through demonstrations is a social activity, one that requires bidirectional communication between a teacher and a student. The work described in this paper studies how the user's visual observation of the robot and the robot's auditory cues affect the user's ability to teach the robot in a social setting. Results show that auditory cues provide important knowledge about the robot's internal state, while visual observation of a robot can hinder an instructor due to incorrect mental models of the robot and distractions from the robot's movements.},
author = {Koenig, Nathan and Takayama, Leila and Matari\'{c}, Maja},
doi = {10.1016/j.neunet.2010.06.005},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Acoustic Stimulation,Adult,Artificial Intelligence,Communication,Computer Graphics,Cues,Feedback, Psychological,Female,Humans,Knowledge,Learning,Learning: physiology,Male,Middle Aged,Neuropsychological Tests,Photic Stimulation,Robotics,Social Environment,Teaching,User-Computer Interface,Young Adult},
number = {8-9},
pages = {1104--12},
pmid = {20598503},
title = {{Communication and knowledge sharing in human-robot interaction and learning from demonstration.}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608010001188},
volume = {23},
year = {2010}
}
@inproceedings{Kormushev2010,
abstract = {We present an approach allowing a robot to acquire new motor skills by learning the couplings across motor control variables. The demonstrated skill is first encoded in a compact form through a modified version of Dynamic Movement Primitives (DMP) which encapsulates correlation information. Expectation-Maximization based Reinforcement Learning is then used to modulate the mixture of dynamical systems initialized from the user's demonstration. The approach is evaluated on a torque-controlled 7 DOFs Barrett WAM robotic arm. Two skill learning experiments are conducted: a reaching task where the robot needs to adapt the learned movement to avoid an obstacle, and a dynamic pancake-flipping task.},
author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G.},
booktitle = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
doi = {10.1109/IROS.2010.5649089},
isbn = {9781424466757},
issn = {2153-0858},
pages = {3232--3237},
title = {{Robot motor skill coordination with EM-based reinforcement learning}},
year = {2010}
}
@article{Kroemer2010,
abstract = {Grasping an object is a task that inherently needs to be treated in a hybrid fashion. The system must decide both where and how to grasp the object. While selecting where to grasp requires learning about the object as a whole, the execution only needs to reactively adapt to the context close to the grasp's location. We propose a hierarchical controller that reflects the structure of these two sub-problems, and attempts to learn solutions that work for both. A hybrid architecture is employed by the controller to make use of various machine learning methods that can cope with the large amount of uncertainty inherent to the task. The controller's upper level selects where to grasp the object using a reinforcement learner, while the lower level comprises an imitation learner and a vision-based reactive controller to determine appropriate grasping motions. The resulting system is able to quickly learn good grasps of a novel object in an unstructured environment, by executing smooth reaching motions and preshaping the hand depending on the object's geometry. The system was evaluated both in simulation and on a real robot. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Kroemer, O. B. and Detry, R. and Piater, J. and Peters, J.},
doi = {10.1016/j.robot.2010.06.001},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Imitation learning,Reactive motion control,Reinforcement learning,Robot grasping},
number = {9},
pages = {1105--1116},
title = {{Combining active learning and reactive control for robot grasping}},
volume = {58},
year = {2010}
}
@article{Kuniyoshi1996,
abstract = {There are currently two major approaches to robot teaching: explicitly tell the robot what to do (programming) or let the robot figure it out for itself (reinforcement learning/genetic algorithms) . In this paper we give an overview of a new approach, in which the robot instead learns novel behaviours by observing the behaviour of others: imitation learning. We summarize the psychological background of this approach, propose a definition of imitation, and identify the important issues involved},
author = {Kuniyoshi, Yasuo and Bakker, P and Bakker, P and Kuniyoshi, Y},
journal = {Proceedings of the AISB96 Workshop on Learning in Robots and Animals},
pages = {3--11},
title = {{Robot see, robot do: An overview of robot imitation}},
year = {1996}
}
@article{Lee2011,
abstract = {This video presents our recent research on the integration of physical human-robot interaction (pHRI) into imitation learning. First, a marker control approach for real-time human motion imitation is shown. Secondly, physical coaching in addition to observational learning is applied for the incremental learning of motion primitives. Last, we extend imitation learning to learning pHRI which includes the establishment of intended physical contacts. The proposed methods were implemented and tested using the IRT humanoid robot and DLR's humanoid upper-body robot Justin.},
author = {Lee, Dongheui and Ott, Christian and Nakamura, Yoshihiko and Hirzinger, Gerd},
doi = {10.1109/ICRA.2011.5979792},
file = {:home/bastian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2011 - Physical Human Robot Interaction in Imitation Learning.pdf:pdf},
isbn = {9781612843803},
journal = {Robotics},
pages = {3439--3440},
title = {{Physical Human Robot Interaction in Imitation Learning}},
url = {http://ieeexplore.ieee.org/ielx5/5967842/5979525/05979792.pdf?tp=\&arnumber=5979792\&isnumber=5979525},
year = {2011}
}
@article{Lockerd2004,
abstract = { We view the problem of machine learning as a collaboration between the human and the machine. Inspired by human-style tutelage, we situate the learning problem within a dialog in which social interaction structures the learning experience, providing instruction, directing attention, and controlling the complexity of the task. We present a learning mechanism, implemented on a humanoid robot, to demonstrate that a collaborative dialog framework allows a robot to efficiently learn a task from a human, generalize this ability to a new task configuration, and show commitment to the overall goal of the learned task. We also compare this approach to traditional machine learning approaches.},
author = {Lockerd, A. and Breazeal, C.},
doi = {10.1109/IROS.2004.1389954},
isbn = {0-7803-8463-6},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
title = {{Tutelage and socially guided robot learning}},
volume = {4},
year = {2004}
}
@article{Lutkebohle2009,
abstract = {If robots are to succeed in novel tasks, they must be able to learn from humans. To improve such human-robot interaction, a system is presented that provides dialog structure and engages the human in an exploratory teaching scenario. Thereby, we specifically target untrained users, who are supported by mixed-initiative interaction using verbal and non-verbal modalities. We present the principles of dialog structuring based on an object learning and manipulation scenario. System development is following an interactive evaluation approach and we will present both an extensible, event-based interaction architecture to realize mixed-initiative and evaluation results based on a video-study of the system. We show that users benefit from the provided dialog structure to result in predictable and successful human-robot interaction.},
author = {Lutkebohle, I. and Peltason, J. and Schillingmann, L. and Wrede, B. and Wachsmuth, S. and Elbrechter, C. and Haschke, R.},
doi = {10.1109/ROBOT.2009.5152521},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {2009 IEEE International Conference on Robotics and Automation},
title = {{The curious robot - Structuring interactive robot learning}},
year = {2009}
}
@article{Matari1997,
abstract = {This paper describes a formulation of reinforcement learning that enables learning in noisy, dynamic environemnts such as in the complex concurrent multi-robot learning domain. The methodology involves minimizing the learning space through the use behaviors and conditions, and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators. We experimentally validate the approach on a group of four mobile robots learning a foraging task},
author = {Matari, Maja J and Systems, Complex and Matari\'{c}, M.J.},
doi = {10.1023/A:1008819414322},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {group behavior,multi-agent systems,reinforcement learning,robot learning,robotics},
pages = {73--83},
title = {{Reinforcement learning in the multi-robot domain}},
url = {http://www.springerlink.com/index/K662611455651Q42.pdf},
volume = {4},
year = {1997}
}
@misc{Mataric2001,
abstract = {This paper describes how the use of behaviors as the underlying control representation provides a useful encoding that both lends robustness to control and allows abstraction for handling scaling in learning, focusing on multi-agent/robot systems.We first define situatedness and embodiment, two key concepts in behavior-based systems (BBS), and then define BBS in detail and contrast it with alternatives, namely reactive, deliberative, and hybrid control. The paper ten focuses on the role and power of behaviors as a representational substrate in learning policies and models, as well as learning from other agents (by demonstration and imitation). We overview a variety of methods we have demonstrated for learning in the multi-robot problem domain.},
author = {Matari\'{c}, Maja J.},
booktitle = {Cognitive Systems Research},
doi = {10.1016/S1389-0417(01)00017-1},
issn = {13890417},
number = {1},
pages = {81--93},
pmid = {781},
title = {{Learning in behavior-based multi-robot systems: policies, models, and other agents}},
volume = {2},
year = {2001}
}
@inproceedings{Mohammad2009,
abstract = {Human-robot interaction using free hand gestures is gaining more importance as more untrained humans are operating robots in home and office environments. The robot needs to solve three problems to be operated by free hand gestures: gesture (command) detection, action generation (related to the domain of the task) and association between gestures and actions. In this paper we propose a novel technique that allows the robot to solve these three problems together learning the action space, the command space, and their relations by just watching another robot operated by a human operator. The main technical contribution of this paper is the introduction of a novel algorithm that allows the robot to segment and discover patterns in its perceived signals without any prior knowledge of the number of different patterns, their occurrences or lengths. The second contribution is using a Ganger-causality based test to limit the search space for the delay between actions and commands utilizing their relations and taking into account the autonomy level of the robot. The paper also presents a feasibility study in which the learning robot was able to predict actor's behavior with 95.2\% accuracy after monitoring a single interaction between a novice operator and a WOZ operated robot representing the actor.},
author = {Mohammad, Yasser and Nishida, Toyoaki and Okada, Shogo},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
doi = {10.1109/IROS.2009.5353987},
isbn = {9781424438044},
pages = {2537--2544},
title = {{Unsupervised simultaneous learning of gestures, actions and their associations for human-robot interaction}},
year = {2009}
}
@phdthesis{Moore1990,
abstract = {This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors-an approach which is formalized here as the AB (State-Action-Behaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall...},
author = {Moore, Andrew W},
booktitle = {Learning},
number = {October},
pages = {42},
title = {{Efficient Memory-based Learning for Robot Control}},
year = {1990}
}
@article{Morimoto2001,
abstract = {In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level. We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor-critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation. ?? 2001 Elsevier Science B.V.},
author = {Morimoto, J. and Doya, K.},
doi = {10.1016/S0921-8890(01)00113-0},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Hierarchical,Motor control,Real robot,Reinforcement learning,Stand-up},
number = {1},
pages = {37--51},
title = {{Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning}},
volume = {36},
year = {2001}
}
@article{Mulling2013,
abstract = {Learning new motor tasks from physical interactions is an important goal for both robotics and machine learning. However, when moving beyond basic skills, most monolithic machine learning approaches fail to scale. For more complex skills, methods that are tailored for the domain of skill learning are needed. In this paper, we take the task of learning table tennis as an example and present a new framework that allows a robot to learn cooperative table tennis from physical interaction with a human. The robot first learns a set of elementary table tennis hitting movements from a human table tennis teacher by kinesthetic teach-in, which is compiled into a set of motor primitives represented by dynamical systems. The robot subsequently generalizes these movements to a wider range of situations using our mixture of motor primitives approach. The resulting policy enables the robot to select appropriate motor primitives as well as to generalize between them. Finally, the robot plays with a human table tennis partner and learns online to improve its behavior. We show that the resulting setup is capable of playing table tennis using an anthropomorphic robot arm.},
author = {M\"{u}lling, Katharina and Kober, Jens and Kroemer, Oliver and Peters, Jan},
doi = {10.1177/0278364912472380},
isbn = {0278-3649},
issn = {0278-3649, 1741-3176},
journal = {The International Journal of Robotics Research},
number = {3},
pages = {263--279},
title = {{Learning to select and generalize striking movements in robot table tennis}},
url = {http://ijr.sagepub.com/content/32/3/263$\backslash$nhttp://ijr.sagepub.com/content/32/3/263.full.pdf},
volume = {32},
year = {2013}
}
@article{Nagai2009,
abstract = {A difficulty in robot action learning is that robots do not know where to attend when observing action demonstration. Inspired by human parent-infant interaction, we suggest that parental action demonstration to infants, called <i>motionese</i>, can scaffold robot learning as well as infants'. Since infants' knowledge about the context is limited, which is comparable to robots, parents are supposed to properly guide their attention by emphasizing the important aspects of the action. Our analysis employing a bottom-up attention model revealed that motionese has the effects of highlighting the initial and final states of the action, indicating significant state changes in it, and underlining the properties of objects used in the action. Suppression and addition of parents' body movement and their frequent social signals to infants produced these effects. Our findings are discussed toward designing robots that can take advantage of parental teaching.},
author = {Nagai, Yukie and Rohlfing, Katharina J.},
doi = {10.1109/TAMD.2009.2021090},
issn = {19430604},
journal = {IEEE Transactions on Autonomous Mental Development},
keywords = {Bottom-up visual attention,Motionese,Parental scaffolding,Robot action learning},
number = {1},
pages = {44--54},
title = {{Computational analysis of motionese toward scaffolding robot action learning}},
volume = {1},
year = {2009}
}
@inproceedings{Natale2007,
abstract = {In this paper we discuss the implementation of a precise reaching controller on an upper-torso humanoid robot. The proposed solution is based on a learning strategy which does not rely on a priori models of the kinematics of the arm nor of that of the head. After learning, the robot can reach for visually identified objects in 3-D space by integrating an open loop and a closed loop component; the open loop controller allows ballistic movements, while the closed loop one performs precise positioning of the hand in visual space. Differently from other approaches we handle the critical case of redundancy in the head and the arm and propose a solution that although preliminary possesses some biological relevance.},
author = {Natale, Lorenzo and Nori, Francesco and Sandini, Giulio and Metta, Giorgio},
booktitle = {2007 IEEE 6th International Conference on Development and Learning, ICDL},
doi = {10.1109/DEVLRN.2007.4354059},
isbn = {1424411165},
keywords = {Development,Humanoid robotics,Learning,Reaching,Visual servoing},
pages = {324--329},
title = {{Learning precise 3D reaching in a humanoid robot}},
year = {2007}
}
@article{Nguyen-Tuong2011,
abstract = {Models are among the most essential tools in robotics, such as kinematics and dynamics models of the robot’s own body and controllable external objects. It is widely believed that intelligent mammals also rely on internal models in order to generate their actions. However, while classical robotics relies on manually generated models that are based on human insights into physics, future autonomous, cognitive robots need to be able to automatically generate models that are based on information which is extracted from the data streams accessible to the robot. In this paper, we survey the progress in model learning with a strong focus on robot control on a kinematic as well as dynamical level. Here, a model describes essential information about the behavior of the environment and the influence of an agent on this environment. In the context of model based learning control, we view the model from three different perspectives. First, we need to study the different possible model learning architectures for robotics. Second, we discuss what kind of problems these architecture and the domain of robotics imply for the applicable learning methods. From this discussion, we deduce future directions of real-time learning algorithms. Third, we show where these scenarios have been used successfully in several case studies.},
author = {Nguyen-Tuong, Dan and Peters, Jan},
doi = {10.1007/s10339-011-0404-1},
isbn = {3405062780},
issn = {1612-4790},
journal = {Cognitive Processing},
pages = {319--340},
pmid = {21487784},
title = {{Model Learning for Robot Control: A Survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=B2DCCC95939ADFB3185C2EEABA0A9EA7?doi=10.1.1.233.4677},
volume = {12},
year = {2011}
}
@misc{Nguyen-Tuong2011a,
abstract = {Models are among the most essential tools in robotics, such as kinematics and dynamics models of the robot's own body and controllable external objects. It is widely believed that intelligent mammals also rely on internal models in order to generate their actions. However, while classical robotics relies on manually generated models that are based on human insights into physics, future autonomous, cognitive robots need to be able to automatically generate models that are based on information which is extracted from the data streams accessible to the robot. In this paper, we survey the progress in model learning with a strong focus on robot control on a kinematic as well as dynamical level. Here, a model describes essential information about the behavior of the environment and the influence of an agent on this environment. In the context of model-based learning control, we view the model from three different perspectives. First, we need to study the different possible model learning architectures for robotics. Second, we discuss what kind of problems these architecture and the domain of robotics imply for the applicable learning methods. From this discussion, we deduce future directions of real-time learning algorithms. Third, we show where these scenarios have been used successfully in several case studies.},
author = {Nguyen-Tuong, Duy and Peters, Jan},
booktitle = {Cognitive Processing},
doi = {10.1007/s10339-011-0404-1},
isbn = {3405062780},
issn = {16124782},
keywords = {Machine learning,Model learning,Regression,Robot control},
number = {4},
pages = {319--340},
pmid = {21487784},
title = {{Model learning for robot control: A survey}},
volume = {12},
year = {2011}
}
@article{Nicolescu2006,
abstract = {Human skill and task teaching is a complex process that relies on multiple means of interaction and learning on the part of the teacher and of the learner. In robotics, however, task teaching has largely been addressed by using a single modality. We present a framework that uses an action-embedded representation to unify interac- tion and imitation in human-robot domains, thereby providing a natural means for robots to interact with and learn from humans. The representation links perception and action in a unique architecture that represents the robot’s skills. The action component allows the use of implicit communication and endows the robot with the ability to convey its intentions through its actions on the environment. The per- ceptual component enables the robot to create a mapping between its observations and its actions and capabilities, allowing it to imitate a task learned from experi- ences of interacting with humans. This chapter describes a system that implements these capabilities and presents validation experiments performed with a Pioneer 2DX mobile robot learning various tasks.},
author = {Nicolescu, M N and Mataric, M J},
doi = {10.1017/CBO9780511489808.027},
isbn = {9780511489808},
journal = {Models and Mechanisms of Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimensions},
keywords = {imitation,interaction},
pages = {407--424},
title = {{Task learning through imitation and human-robot interaction}},
year = {2006}
}
@article{Nicolescu2001,
abstract = {We focus on a robotic domain in which a human acts both as a
teacher and a collaborator to a mobile robot. First, we present an
approach that allows a robot to learn task representations from its own
experiences of interacting with a human. While most approaches to
learning from demonstration have focused on acquiring policies (i.e.,
collections of reactive rules), we demonstrate a mechanism that
constructs high-level task representations based on the robot's
underlying capabilities. Next, we describe a generalization of the
framework to allow a robot to interact with humans in order to handle
unexpected situations that can occur in its task execution. Without
using explicit communication, the robot is able to engage a human to aid
it during certain parts of task execution. We demonstrate our concepts
with a mobile robot learning various tasks from a human and, when
needed, interacting with a human to get help performing them},
author = {Nicolescu, M.N. and Mataric, M.J.},
doi = {10.1109/3468.952716},
isbn = {1083-4427},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
number = {5},
title = {{Learning and interacting in human-robot domains}},
volume = {31},
year = {2001}
}
@article{Nicolescu2001a,
abstract = {In this paper we address the problem of teaching robots to perform
various tasks. We present a behavior-based approach that extends the
capabilities of robots, allowing them to learn representations of
complex tasks from their own experiences of interacting with a human,
and to use the acquired knowledge to teach other robots in turn. A
learner robot follows a human or robot teacher and maps its own
observations of the environment to its internal behaviors, building at
run-time a representation of the experienced task in the form of a
behavior network. To enable this, we introduce an architecture that
allows the representation and execution of complex and flexible
sequences of behaviors and an online algorithm that builds the task
representation from observations. We demonstrate our approach in a set
of human(teacher)-robot(learner) and robot(teacher)-robot(learner)
experiments, in which the robots learn representations for multiple
tasks and are able to execute them even in environments with distractor
objects that could hinder the learning and the execution process},
author = {Nicolescu, M.N. and Mataric, M.J.},
doi = {10.1109/IROS.2001.976257},
isbn = {0-7803-6612-3},
journal = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
title = {{Experience-based representation construction: learning from human
and robot teachers}},
volume = {2},
year = {2001}
}
@inproceedings{Nicolescu2003,
abstract = {Among humans, teaching various tasks is a complex process which relies on multiple means for interaction and learning, both on the part of the teacher and of the learner. Used together, these modalities lead to effective teaching and learning approaches, respectively. In the robotics domain, task teaching has been mostly addressed by using only one or very few of these interactions. In this paper we present an approach for teaching robots that relies on the key features and the general approach people use when teaching each other: first give a demonstration, then allow the learner to refine the acquired capabilities by practicing under the teacher's supervision, involving a small number of trials. Depending on the quality of the learned task, the teacher may either demonstrate it again or provide specific feedback during the learner's practice trial for further refinement. Also, as people do during demonstrations, the teacher can provide simple instructions and informative cues, increasing the performance of learning. Thus, instructive demonstrations, generalization over multiple demonstrations and practice trials are essential features for a successful human-robot teaching approach. We implemented a system that enables all these capabilities and validated these concepts with a Pioneer 2DX mobile robot learning tasks from multiple demonstrations and teacher feedback.},
author = {Nicolescu, Monica N. and Mataric, Maja J.},
booktitle = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent systems - AAMAS '03},
doi = {10.1145/860575.860614},
isbn = {1581136838},
keywords = {human-robot interaction,learning by demonstration,robotics},
pages = {241},
title = {{Natural methods for robot task learning}},
url = {http://dl.acm.org/citation.cfm?id=860575.860614},
year = {2003}
}
@article{Oztop2010,
abstract = {Primates, in particular, humans are very adept at learning to use tools. In this talk, I will introduce a paradigm that utilizes this sensorimotor learning capacity to obtain robot behaviors, which otherwise would require manual programming by experts. The idea is to consider the target robotic platform as a tool that can be controlled by a human. Provided with an intuitive interface for controlling the robot, the human learns to perform a given task using the robot. This is akin to the stage where a beginner is learning to drive a car. After sufficient learning, the skilled control of the robot by the human provides learning data points that are used for constructing an autonomous controller so that the robot can perform the task without human guidance. I will demonstrate the feasibility of this framework by presenting several examples including a manipulation skill obtained for a robotic hand, and statically stable reaching skill obtained for a small humanoid robot. From an engineering point of view, this paradigm relies on techniques from teleoperation and machine learning, and has the same goals with robotic imitation and robot learning by demonstration. The key difference is that the proposed paradigm includes the human in the control loop and employs the human brain as the adaptive controller to accomplish a given task. Once the control proficiency has been attained by the human, then obtaining an autonomous controller boils down to reverse engineering the control policy established by the human brain. As time permits, I will present some ideas on the neural correlates of human-in-the-loop robot control and show how the interfaces built for robot skill synthesis can also be used in the reverse direction for probing motor control mechanisms employed by the central nervous system.},
author = {Oztop, E.},
doi = {10.1109/ROMAN.2010.5598759},
isbn = {978-1-4244-7991-7},
issn = {1944-9445},
journal = {RO-MAN, 2010 IEEE},
title = {{Human sensorimotor learning for robot skill synthesis}},
year = {2010}
}
@article{Park2001,
abstract = {In a multi-agent system, action selection is important for the cooperation and coordination among agents. As the environment is dynamic and complex, modular Q-learning, which is one of the reinforcement learning schemes, is employed in assigning a proper action to an agent in the multi-agent system. The architecture of modular Q-learning consists of learning modules and a mediator module. The mediator module of the modular Q-learning system selects a proper action for the agent based on the Q-value obtained from each learning module. To obtain better performance, along with the Q-value, the mediator module also considers the state information in the action selection process. A uni-vector field is used for robot navigation. In the robot soccer environment, the effectiveness and applicability of modular Q-learning and the uni-vector field method are verified by real experiments using five micro-robots. ?? 2001 Elsevier Science B.V.},
author = {Park, Kui Hong and Kim, Yong Jae and Kim, Jong Hwan},
doi = {10.1016/S0921-8890(01)00114-2},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Action selection,Modular Q-learning,Multi-agent system,Reinforcement learning,Robot soccer system},
number = {2},
pages = {109--122},
title = {{Modular Q-learning based multi-agent cooperation for robot soccer}},
volume = {35},
year = {2001}
}
@incollection{Parker2002,
abstract = {to maximize the coverage. The CM O MMT application offers a rich testbed for research in - cooperation, , and adaptation because it is an   . In addition, many variations },
author = {Parker, Lynne E and Touzet, Claude and Fernandez, Fernando},
booktitle = {Robot Teams: From Diversity to Polymorphism. AK Peters},
pages = {191--236},
title = {{Techniques for Learning in Multi-Robot Teams}},
url = {http://www.plg.inf.uc3m.es/~ffernand/papers/chapter01.pdf},
year = {2002}
}
@inproceedings{Pugh2006,
abstract = {We apply an adapted version of Particle Swarm Optimization to distributed unsupervised robotic learning in groups of robots with only local information. The performance of the learning technique for a simple task is compared across robot groups of various sizes, with the maximum group size allowing each robot to individually contain and manage a single PSO particle. Different PSO neighborhoods based on limitations of real robotic communication are tested in this scenario, and the effect of varying communication power is explored. The algorithms are then applied to a group learning scenario to explore their susceptibility to the credit assignment problem. Results are discussed and future work is proposed.},
author = {Pugh, Jim and Martinoli, Alcherio},
booktitle = {AAMAS '06 Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems},
doi = {10.1145/1160633.1160715},
isbn = {1-59593-303-4},
pages = {441--448},
title = {{Multi-robot learning with particle swarm optimization}},
url = {http://doi.acm.org/10.1145/1160633.1160715},
year = {2006}
}
@article{Pugh2009,
abstract = {Designing effective behavioral controllers for mobile robots can be difficult and tedious; this process can be circumvented by using online learning techniques which allow robots to generate their own controllers online in an automated fashion. In multi-robot systems, robots operating in parallel can potentially learn at a much faster rate by sharing information amongst themselves. In this work, we use an adapted version of the Particle Swarm Optimization algorithm in order to accomplish distributed online robotic learning in groups of robots with access to only local information. The effectiveness of the learning technique on a benchmark task (generating high-performance obstacle avoidance behavior) is evaluated for robot groups of various sizes, with the maximum group size allowing each robot to individually contain and manage a single PSO particle. To increase the realism of the technique, different PSO neighborhoods based on limitations of real robotic communication are tested and compared in this scenario. We explore the effect of varying communication power for one of these communication-based PSO neighborhoods. To validate the effectiveness of these learning techniques, fully distributed online learning experiments are run using a group of 10 real robots, generating results which support the findings from our simulations.},
author = {Pugh, Jim and Martinoli, Alcherio},
doi = {10.1007/s11721-009-0030-z},
issn = {19353812},
journal = {Swarm Intelligence},
keywords = {Multi-robot systems,Particle swarm optimization,Robotic learning},
number = {3},
pages = {203--222},
title = {{Distributed scalable multi-robot learning using particle swarm optimization}},
volume = {3},
year = {2009}
}
@inproceedings{Reiter2012,
abstract = {In this paper, we present a visual learning algorithm for estimating the configuration of a multisegment continuum robot designed for surgery. Our algorithm interpolates a stereo visual feature descriptor manifold using Radial Basis Functions (RBFs) to estimate configuration pose angles. Results are shown on a 3-segment snake robot, where rotational accuracy in the range of 1 -2 is achieved.},
author = {Reiter, Austin and Bajo, Andrea and Iliopoulos, Konstantinos and Simaan, Nabil and Allen, Peter K.},
booktitle = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
doi = {10.1109/BioRob.2012.6290702},
isbn = {9781457711992},
issn = {21551774},
pages = {829--834},
title = {{Learning-based configuration estimation of a multi-segment continuum robot}},
year = {2012}
}
@article{Riedmiller2009,
abstract = {Batch reinforcement learning methods provide a powerful framework for learning efficiently and effectively in autonomous robots. The paper reviews some recent work of the authors aiming at the successful application of reinforcement learning in a challenging and complex domain. It discusses several variants of the general batch learning framework, particularly tailored to the use of multilayer perceptrons to approximate value functions over continuous state spaces. The batch learning framework is successfully used to learn crucial skills in our soccer-playing robots participating in the RoboCup competitions. This is demonstrated on three different case studies.},
author = {Riedmiller, Martin and Gabel, Thomas and Hafner, Roland and Lange, Sascha},
doi = {10.1007/s10514-009-9120-4},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Autonomous learning robots,Batch reinforcement learning,Learning mobile robots,Neural control,RoboCup},
number = {1},
pages = {55--73},
title = {{Reinforcement learning for robot soccer}},
volume = {27},
year = {2009}
}
@article{Rouanet2010,
abstract = {We developed three interfaces to allow non-expert users to teach name for new visual objects and compare them through user's studies in term of learning efficiency.},
author = {Rouanet, P. and Oudeyer, P.-Y. and Filliat, D.},
doi = {10.1109/HRI.2010.5453202},
isbn = {978-1-4244-4892-0},
journal = {Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on},
keywords = {Human-Robot interaction,interfaces,joint attention,learning,social robotics},
title = {{A study of three interfaces allowing non-expert users to teach new visual objects to a robot and their impact on learning efficiency}},
year = {2010}
}
@article{Rozo2011,
abstract = {Researchers are becoming aware of the importance of other information sources besides visual data in robot learning by demonstration (LbD). Force- based perceptions are shown to convey very rele- vant information missed by visual and position sensors for learning specific tasks. In this pa- per, we review some recent works using forces as input data in LbD and Human-Robot interaction (HRI) scenarios, and propose a complete learning framework for teaching force-based manipulation skills to a robot through a haptic device. We sug- gest to use haptic interfaces not only as a demon- stration tool but also as a communication chan- nel between the human and the robot, getting the teacher more involved in the teaching process by experiencing the force signals sensed by the robot. Within the proposed framework, we provide solutions for treating force signals, extracting relevant information about the task, encoding the training data and generalizing to perform successfully under unknown conditions. This work aims to put forth a different way of considering human-robot interac- tion involving force perceptions in the process.},
author = {Rozo, Leonel and Jim, Pablo and Rob, Institut De},
file = {:home/bastian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rozo, Jim, Rob - 2011 - Robot Learning from Demonstration in the Force Domain.pdf:pdf},
journal = {Communications},
pages = {1--6},
title = {{Robot Learning from Demonstration in the Force Domain}},
url = {http://upcommons.upc.edu/e-prints/handle/2117/14106},
year = {2011}
}
@inproceedings{Salter2006,
abstract = {If we are to achieve natural human-robot interaction, we may need to complement current vision and speech interfaces. Touch may provide us with an extra tool in this quest. In this paper we demonstrate the role of touch in interaction between a robot and a human. We show how infrared sensors located on robots can be easily used to detect and distinguish human interaction, in this case interaction with individual children. This application of infrared sensors potentially has many uses; for example, in entertainment or service robotics. This system could also benefit therapy or rehabilitation, where the observation and recording of movement and interaction is important. In the long term, this technique might enable robots to adapt to individuals or individual types of user.},
author = {Salter, Tamie and Dautenhahn, Kerstin and {Te Boekhorst}, Ren\'{e}},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2005.09.022},
isbn = {09218890},
issn = {09218890},
keywords = {Adaptation,Natural human-robot interaction,Patterns of behaviour,Touch},
number = {2},
pages = {127--134},
title = {{Learning about natural human-robot interaction styles}},
volume = {54},
year = {2006}
}
@inproceedings{Saponaro2011,
abstract = {We propose a mechanism to communicate emotions to humans by using head, torso and arm movements of a humanoid robot, without exploiting its facial features. To this end, we build a library of pre-programmed robot movements and we ask people to attribute emotional scores to these initial movements. The answers are then used to fine-tune motion parameters with an active learning approach.},
author = {Saponaro, G and Bernardino, A},
booktitle = {Human-Robot Interaction (HRI), 2011 6th ACM/IEEE International Conference on},
doi = {10.1145/1957656.1957752},
isbn = {2167-2121},
pages = {243--244},
title = {{Generation of meaningful robot expressions with active learning}},
year = {2011}
}
@article{Schaal2000,
abstract = {Locally weighted learning (LWL) is a class of statistical learning
techniques that provides useful representations and training algorithms
for learning about complex phenomena during autonomous adaptive control
of robotic systems. This paper introduces several LWL algorithms that
have been tested successfully in real-time learning of complex robot
tasks. We discuss two major classes of LWL, memory-based LWL and purely
incremental LWL that does not need to remember any data explicitly. In
contrast to the traditional beliefs that LWL methods cannot work well in
high-dimensional spaces, we provide new algorithms that have been tested
in up to 50 dimensional learning problems. The applicability of our LWL
algorithms is demonstrated in various robot learning examples, including
the learning of devil-sticking, pole-balancing of a humanoid robot arm,
and inverse-dynamics learning for a seven degree of-freedom robot},
author = {Schaal, S. and Atkeson, C.G. and Vijayakumar, S.},
doi = {10.1109/ROBOT.2000.844072},
isbn = {0-7803-5886-4},
issn = {1050-4729},
journal = {Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
pmid = {164},
title = {{Real-time robot learning with locally weighted statistical learning}},
volume = {1},
year = {2000}
}
@article{Schaal1997,
abstract = {By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely at- tempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based rein- forcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.},
author = {Schaal, Stefan},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
number = {9},
pages = {1040--1046},
title = {{Robot learning from demonstration}},
url = {http://wwwiaim.ira.uka.de/users/rogalla/WebOrdnerMaterial/ml-robotlearning.pdf},
year = {1997}
}
@article{Schaal2002,
abstract = {Learning robot control, a subclass of the field of learning control, refers to the process of acquiring a sensory-motor control strategy for a particular movement task and movement system by trial and error. Learning control is usually distinguished from adaptive control (see ADAPTIVE CONTROL) in that the learning system is permitted to fail during the process of learning, while adaptive control emphasizes single trial convergence without failure. Thus, learning control resembles the way that humans and animals acquire new movement strategies, while adaptive control is a special case of learning control that fulfills stringent performance constraints, e.g., as needed in life-critical systems like airplanes and industrial robots.},
author = {Schaal, Stefan},
journal = {Learning},
pages = {983--987},
title = {{Learning Robot Control}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.4636\&amp;rep=rep1\&amp;type=pdf},
volume = {2},
year = {2002}
}
@article{Schaal1994,
abstract = {Issues involved in implementing robot learning for a challenging
dynamic task are explored in this article, using a case study from robot
juggling. We use a memory-based local modeling approach (locally
weighted regression) to represent a learned model of the task to be
performed. Statistical tests are given to examine the uncertainty of a
model, to optimize its prediction quality, and to deal with noisy and
corrupted data. We develop an exploration algorithm that explicitly
deals with prediction accuracy requirements during exploration. Using
all these ingredients in combination with methods from optimal control,
our robot achieves fast real-time learning of the task within 40 to 100
trials},
author = {Schaal, Stefan and Atkeson, Christopher G.},
doi = {10.1109/37.257895},
issn = {02721708},
journal = {IEEE Control Systems Magazine},
number = {1},
pages = {57--71},
title = {{Robot juggling: implementation of memory-based learning}},
volume = {14},
year = {1994}
}
@article{Schaal2010,
abstract = {Recent trends in robot learning are to use trajectory-based optimal control techniques and reinforcement learning to scale complex robotic systems. On the one hand, increased computational power and multiprocessing, and on the other hand, probabilistic reinforcement learning methods and function approximation, have contributed to a steadily increasing interest in robot learning. Imitation learning has helped significantly to start learning with reasonable initial behavior. However, many applications are still restricted to rather lowdimensional domains and toy applications. Future work will have to demonstrate the continual and autonomous learning abilities, which were alluded to in the introduction.},
author = {Schaal, Stefan and Atkeson, Christopher G.},
doi = {10.1109/MRA.2010.936957},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Learning control,Optimal control,Reinforcement learning,Robot learning},
number = {2},
pages = {20--29},
title = {{Learning control in robotics}},
volume = {17},
year = {2010}
}
@article{Schaal2002a,
abstract = {Locally weighted learning (LWL) is a class of techniques from nonparametric statistics that provides useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of robotic systems. This paper introduces several LWL algorithms that have been tested successfully in real-time learning of complex robot tasks. We discuss two major classes of LWL, memory-based LWL and purely incremental LWL that does not need to remember any data explicitly. In contrast to the traditional belief that LWL methods cannot work well in high-dimensional spaces, we provide new algorithms that have been tested on up to 90 dimensional learning problems. The applicability of our LWL algorithms is demonstrated in various robot learning examples, including the learning of devil-sticking, pole-balancing by a humanoid robot arm, and inverse-dynamics learning for a seven and a 30 degree-of-freedom robot. In all these examples, the application of our statistical neural networks techniques allowed either faster or more accurate acquisition of motor control than classical control engineering.},
author = {Schaal, Stefan and Atkeson, Christopher G. and Vijayakumar, Sethu},
doi = {10.1023/A:1015727715131},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {Incremental learning,Internal models,Locally weighted learning,Motor control,Nonparametric regression},
number = {1},
pages = {49--60},
title = {{Scalable techniques from nonparametric statistics for real time robot learning}},
volume = {17},
year = {2002}
}
@article{Sim2003,
abstract = {This paper considers the fundamental issues of robot learning in which answers to basic questions on robot learning, such as "What can the robot learn?", "What are the consequences of robot learning?", "How does the robot learn?", "How fast do robots need to learn?", and "When do robots learn?" are addressed. The answers to these questions may lead to the identification of the elements of robot learning and the interaction between these elements. Hence, the purpose of this paper is to discuss the fundamental issues in a holistic manner so that key elements that characterise robot learning can be formalised into a framework.},
author = {Sim, Siang Kok Sim Siang Kok and Ong, Kai Wei Ong Kai Wei and Seet, G.},
doi = {10.1109/ICCA.2003.1595102},
isbn = {0-7803-7777-X},
journal = {2003 4th International Conference on Control and Automation Proceedings},
title = {{A Foundation for Robot Learning}},
year = {2003}
}
@inproceedings{Steil2004,
abstract = {A key prerequisite to make user instruction of work tasks by interactive demonstration effective and convenient is situated multi-modal interaction aiming at an enhancement of robot learning beyond simple low-level skill acquisition. We report the status of the Bielefeld GRAVIS-robot system that combines visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation to allow multi-modal task-oriented instructions. With respect to this platform, we discuss the essential role of learning for robust functioning of the robot and sketch the concept of an integrated architecture for situated learning on the system level. It has the long-term goal to demonstrate speech-supported imitation learning of robot actions. We describe the current state of its realization to enable imitation of human hand postures for flexible grasping and give quantitative results for grasping a broad range of everyday objects. © 2004 Elsevier B.V. All rights reserved.},
author = {Steil, J. J. and R̈othling, F. and Haschke, R. and Ritter, H.},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.007},
isbn = {09218890},
issn = {09218890},
keywords = {Architecture,Grasping,Imitation,Interactive demonstration,Learning},
number = {2-3},
pages = {129--141},
title = {{Situated robot learning for multi-modal instruction and imitation of grasping}},
volume = {47},
year = {2004}
}
@article{Tani2002,
abstract = {In this paper, the processes of exploration and of incremental learning in the robot navigation task are studied using the dynamical systems approach. A neural network model which performs the forward modeling, planning, consolidation learning and novelty rewarding is used for the robot experiments. Our experiments showed that the robot repeated a few variations of travel patterns in the beginning of the exploration, and later the robot explored more diversely in the workspace by combining and mutating the previously experienced patterns. Our analysis indicates that internal confusion due to immature learning plays the role of a catalyst in generating diverse action sequences. It is found that these diverse exploratory travels enable the robot to acquire adequate modeling of the environment in the end. © 2002 Elsevier Science B.V. All rights reserved.},
author = {Tani, Jun and Yamamoto, Jun},
doi = {10.1016/S1389-0417(02)00052-9},
isbn = {1389-0417},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Exploration,Learning,Robot},
number = {3},
pages = {459--470},
title = {{On the dynamics of robot exploration learning}},
volume = {3},
year = {2002}
}
@article{Thomaz2010,
abstract = {In this paper we provide a brief overview of our research agenda in Human-Robot Interaction and Interactive Learning. We highlight key components to be demonstrated as part of the CHI 2010 Media Showcase.},
author = {Thomaz, Andrea L},
doi = {10.1145/1753846.1753912},
isbn = {9781605589305},
journal = {Learning},
keywords = {human robot interaction,socially guided machine},
pages = {3037--3040},
title = {{Interactive Robot Task Learning}},
url = {http://portal.acm.org/citation.cfm?doid=1753846.1753912},
year = {2010}
}
@inproceedings{Thomaz2007,
abstract = {We present a learning mechanism, Socially Guided Exploration, in which a robot learns new tasks through a combination of self-exploration and social interaction. The system's motivational drives (novelty, mastery), along with social scaffolding from a human partner, bias behavior to create learning opportunities for a Reinforcement Learning mechanism. The system is able to learn on its own, but can flexibly use the guidance of a human partner to improve performance. An initial experiment shows how a human shapes the learning process through suggesting actions, drawing attention to goal states,},
author = {Thomaz, Andrea L. and Breazeal, Cynthia},
booktitle = {2007 IEEE 6th International Conference on Development and Learning, ICDL},
doi = {10.1109/DEVLRN.2007.4354078},
isbn = {1424411165},
pages = {82--87},
title = {{Robot learning via socially guided exploration}},
year = {2007}
}
@misc{Thrun1998,
abstract = {Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.},
author = {Thrun, Sebastian},
booktitle = {Artificial Intelligence},
doi = {10.1016/S0004-3702(97)00078-7},
isbn = {0004-3702},
issn = {00043702},
number = {1},
pages = {21--71},
title = {{Learning metric-topological maps for indoor mobile robot navigation}},
volume = {99},
year = {1998}
}
@misc{Thrun1995,
abstract = {To estimate the short-term results of robot-assisted laparoscopic radical prostatectomy (RALRP) during the learning curve, in terms of surgical, oncological and functional outcomes, we conducted a prospective survey on RALRP. From July 2007, a single surgeon performed 63 robotic prostatectomies using the same operative technique. Perioperative data, including pathological and early functional results of the patient, were collected prospectively and analyzed. Along with the accumulation of the cases, the total operative time, setup time, console time and blood loss were significantly decreased. No major complication was present in any patient. Transfusion was needed in six patients; all of them were within the initial 15 cases. The positive surgical margin rate was 9.8\% (5/51) in pT2 disease. The most frequent location of positive margin in this stage was the lateral aspect (60\%), but in pT3 disease multiple margins were the most frequent (41.7\%). Overall, 53 (84.1\%) patients had totally continent status and the median time to continence was 6.56 weeks. Among 17 patients who maintained preoperative sexual activity (Sexual Health Inventory for Men >/= 17), stage below pT2, followed up for > 6 months with minimally one side of neurovascular bundle preservation procedure, 12 (70.6\%) were capable of intercourse postoperatively, and the mean time for sexual intercourse after operation was 5.7 months. In this series, robotic prostatectomy was a feasible and reproducible technique, with a short learning curve and low perioperative complication rate. Even during the initial phase of the learning curve, satisfactory results were obtained with regard to functional and oncological outcome.Asian Journal of Andrology (2009) 11: 167-175. doi: 10.1038/aja.2008.52; published online 19 January 2009.},
author = {Thrun, Sebastian and Mitchell, Tom M.},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/0921-8890(95)00004-Y},
isbn = {0921-8890},
issn = {09218890},
number = {1-2},
pages = {25--46},
title = {{Lifelong robot learning}},
volume = {15},
year = {1995}
}
@inproceedings{Uri2007,
abstract = {This paper presents a collaborative reinforcement learning algorithm, CQ(lambda), designed to accelerate learning by integrating a human operator into the learning process. The CQ(lambda) -learning algorithm enables collaboration of knowledge between the robot and a human; the human, responsible for remotely monitoring the robot, suggests solutions when intervention is required. Based on its learning performance, the robot switches between fully autonomous operation, and the integration of human commands. The CQ(lambda) -learning algorithm was tested on a Motoman UP-6 fixed-arm robot required to empty the contents of a suspicious bag. Experimental results of comparing the CQ(lambda) with the standard Q(lambda), indicated the superiority of the CQ(lambda) while achieving an improvement of 21.25\% in the average reward.},
author = {Uri, Kartoun and Helman, Stern and Yael, Edan},
booktitle = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2006.384802},
isbn = {1424401003},
issn = {1062922X},
pages = {4249--4255},
title = {{Human-robot collaborative learning system for inspection}},
volume = {5},
year = {2007}
}
@article{Vlassis2009,
abstract = {Abstract We address the problem of learning robot control by model-free reinforcement learning (RL). We adopt the probabilistic model for model-free RL of Vlassis and Toussaint (Proceedings of the international conference on machine learning, Montreal, Canada, 2009), and we propose a Monte Carlo EM algorithm (MCEM) for control learning that searches directly in the space of controller parameters using information obtained from randomly generated robot trajectories. MCEM is related to, and generalizes, the PoWER algorithm of Kober and Peters (Proceedings of the neural information processing systems, 2009). In the finite-horizon case MCEM reduces precisely to PoWER, but MCEM can also handle the discounted infinite-horizon case. An interesting result is that the infinite-horizon case can be viewed as a randomized version of the finite-horizon case, in the sense that the length of each sampled trajectory is a random draw from an appropriately constructed geometric distribution. We provide some preliminary experiments demonstrating the effects of fixed (PoWER) vs randomized (MCEM) horizon length in two simulated and one real robot control tasks.},
author = {Vlassis, Nikos and Toussaint, Marc and Kontes, Georgios and Piperidis, Savas},
doi = {10.1007/s10514-009-9132-0},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {EM algorithm,Model-free robot control,Probabilistic inference,Reinforcement learning},
number = {2},
pages = {123--130},
title = {{Learning Model-free robot control by a Monte Carlo em algorithm}},
volume = {27},
year = {2009}
}
@article{Wang2008,
abstract = {This paper presents a machine-learning approach to the multi-robot coordination problem in an unknown dynamic environment. A multi-robot object transportation task is employed as the platform to assess and validate this approach. Specifically, a flexible two-layer multi-agent architecture is developed to implement multi-robot coordination. In this architecture, four software agents form a high-level coordination subsystem while two heterogeneous robots constitute the low-level control subsystem. Two types of machine learning-reinforcement learning (RL) and genetic algorithms (GAs)-are integrated to make decisions when the robots cooperatively transport an object to a goal location while avoiding obstacles. A probabilistic arbitrator is used to determine the winning output between the RL and GA algorithms. In particular, a modified RL algorithm called the sequential Q-learning algorithm is developed to deal with the issues of behavior conflict that arise in multi-robot cooperative transportation tasks. The learning-based high-level coordination subsystem sends commands to the low-level control subsystem, which is implemented with a hybrid force/position control scheme. Simulation and experimental results are presented to demonstrate the effectiveness and adaptivity of the developed approach. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Wang, Ying and de Silva, Clarence W.},
doi = {10.1016/j.engappai.2007.05.006},
isbn = {0952-1976},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Autonomous robots,Cooperative control,Genetic algorithms,Intelligent transportation,Multi-agent systems,Multi-robot systems,Q-learning},
number = {3},
pages = {470--484},
title = {{A machine-learning approach to multi-robot coordination}},
volume = {21},
year = {2008}
}
@inproceedings{Wermter2004,
abstract = {Learning by multimodal observation of vision and language offers a potentially powerful paradigm for robot learning. Recent experiments have shown that 'mirror' neurons are activated when an action is being performed, perceived, or verbally referred to. Different input modalities are processed by distributed cortical neuron ensembles for leg, arm and head actions. In this overview paper we consider this evidence from mirror neurons by integrating motor, vision and language representations in a learning robot. © 2004 Elsevier B.V. All rights reserved.},
author = {Wermter, S. and Weber, C. and Elshaw, M. and Panchev, C. and Erwin, H. and Pulverm̈uller, F.},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.011},
issn = {09218890},
keywords = {Language,Learning robots,Multimodal integration,Neural networks,Vision},
number = {2-3},
pages = {171--175},
title = {{Towards multimodal neural robot learning}},
volume = {47},
year = {2004}
}
@article{Yang2004,
abstract = {Multiagent reinforcement learning for multi- robot systems is a challenging issue in both robotics and artificial intelligence. With the ever increasing interests in theoretical researches and practical applications, currently there have been a lot of efforts towards providing some solutions to this challenge. However, there are still many difficulties in scaling up the multiagent reinforcement learning to multi-robot systems. The main objective of this paper is to provide a survey, though not completely on the multiagent reinforcement learning in multi-robot systems. After reviewing important advances in this field, some challenging problems and promising research directions are analyzed. A concluding remark is made from the perspectives of the authors. I.},
author = {Yang, Erfu and Gu, Dongbing},
doi = {10.1.1.2.1602},
isbn = {1-4244-0341-3},
journal = {University of Essex Technical Report CSM-404, \ldots},
pages = {1--23},
title = {{Multiagent reinforcement learning for multi-robot systems: A survey}},
url = {http://computerscience.nl/docs/vakken/aibop/MAS-reinforcement-learning.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.1602\&rep=rep1\&type=pdf$\backslash$npapers2://publication/uuid/7E7FE019-FB17-4EB1-82E1-B90BE22E856A},
year = {2004}
}
